{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor, LongTensor\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from data import  CornellVocab, CornellMovie, OpenSubVocab, OpenSub, sort_batch\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from utils import parse, length_to_mask, masked_cross_entropy_loss, save_checkpoint\n",
    "from tensorboardX import SummaryWriter\n",
    "from seq import Encoder, Decoder, run\n",
    "from inference import Beam\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "from time import strftime, localtime, time\n",
    "from torch.nn.functional import softmax, log_softmax\n",
    "import progressbar\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"epoch\":20,\n",
    "    \"batch_size\":2,\n",
    "    \"num_workers\":4,\n",
    "    \"train_path\":\"../data/train/2020_train\",\n",
    "    \"test_path\":\"../data/dev/2020_dev\",\n",
    "    \"vocab_size\":25000,\n",
    "    \"embed_size\":1000,\n",
    "    \"hidden_size\":1000,\n",
    "    \"num_layers\":4,\n",
    "    \"clip_thresh\":1,\n",
    "    \"seed\":1,\n",
    "    \"lr\":0.1,\n",
    "    \"global_max_target_len\":20,\n",
    "    \"cuda\":torch.cuda.is_available,\n",
    "    \"resume\":\"checkpoint-29999\",\n",
    "    \"dir\":\"final\",\n",
    "    \"reverse\": False,\n",
    "    \"vocab_path\":\"../data/movie_25000\",\n",
    "    \"dropout\":0.2,\n",
    "    \"beam_size\":5\n",
    "}\n",
    "class AttributeDict(dict):\n",
    "    def __getattr__(self, attr):\n",
    "        return self[attr]\n",
    "    def __setattr__(self, attr, value):\n",
    "        self[attr] = value\n",
    "args = AttributeDict(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start model building, you are using cuda.\n",
      "finish data loading.\n"
     ]
    }
   ],
   "source": [
    "assert args.resume is not None\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda: torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "cuda_prompt = \"you are using cuda.\" if args.cuda else \"you are not using cuda.\"\n",
    "print(\"start model building, \"+cuda_prompt)\n",
    "vocab = OpenSubVocab(args.vocab_path)\n",
    "test_data = OpenSub(args, vocab, args.test_path)\n",
    "test_loader = torch.utils.data.DataLoader(test_data,\n",
    "                                          batch_size=args.batch_size,\n",
    "                                          shuffle=True, collate_fn=sort_batch,\n",
    "                                          num_workers=args.num_workers, pin_memory=True)\n",
    "print(\"finish data loading.\")\n",
    "\n",
    "#beam_search(encoder, decoder, test_loader, args.beam_size, vocab, args.alpha, args.n_best, 20, args.cuda, None)\n",
    "# greedy(test_loader, encoder, decoder, args.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(args, test_data.source_vocab.vocab_size).cuda() if args.cuda else Encoder(args, test_data.source_vocab.vocab_size)\n",
    "decoder = Decoder(args, test_data.target_vocab.vocab_size).cuda() if args.cuda else Decoder(args, test_data.target_vocab.vocab_size)\n",
    "checkpoint = torch.load(os.path.join(args.dir, args.resume))\n",
    "encoder.load_state_dict(checkpoint['encoder_state'])\n",
    "decoder.load_state_dict(checkpoint['decoder_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_reverse = Encoder(args, test_data.source_vocab.vocab_size).cuda() if args.cuda else Encoder(args, test_data.source_vocab.vocab_size)\n",
    "decoder_reverse = Decoder(args, test_data.target_vocab.vocab_size).cuda() if args.cuda else Decoder(args, test_data.target_vocab.vocab_size)\n",
    "checkpoint = torch.load(os.path.join(\"final_reverse\", args.resume))\n",
    "encoder_reverse.load_state_dict(checkpoint['encoder_state'])\n",
    "decoder_reverse.load_state_dict(checkpoint['decoder_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "def ngram_precision(prediction, reference, n):\n",
    "    \"\"\"\n",
    "    predictions: T list\n",
    "    reference: Txn list\n",
    "    \"\"\"\n",
    "    prediction = prediction.view(-1).tolist()\n",
    "    reference = reference.view(-1).tolist()\n",
    "    p_ngrams = [tuple(prediction[i:i+n]) for i in range(len(prediction)+1-n)]\n",
    "    r_ngrams = [tuple(reference[i:i+n]) for i in range(len(reference)+1-n)]\n",
    "    p_ngrams_counts = Counter(p_ngrams)\n",
    "    r_ngrams_counts = Counter(r_ngrams)\n",
    "    overlap = p_ngrams_counts & r_ngrams_counts\n",
    "    overlap_count = sum(overlap.values())\n",
    "    total_count = len(r_ngrams_counts)\n",
    "    return overlap_count, total_count\n",
    "\n",
    "def get_stats(hypothesis, hypo_lengths, reference, refer_lengths, n=4):\n",
    "    c = sum(hypo_lengths)\n",
    "    r = sum(refer_lengths)\n",
    "    batch_size = hypothesis.size()[1]\n",
    "    stats = torch.zeros(batch_size, 2*n)\n",
    "    for b in range(batch_size):\n",
    "        for i in range(n):\n",
    "            overlap, total = ngram_precision(hypothesis[0:hypo_lengths[b], b],\n",
    "                                             reference[0:refer_lengths[b], b],\n",
    "                                             i+1)\n",
    "            stats[b, 2*i] = overlap\n",
    "            stats[b, 2*i+1] = total\n",
    "    stats = torch.sum(stats, 0)\n",
    "    return stats, c, r\n",
    "def compute_bleu(n, c, r, stats):\n",
    "    \"\"\"\n",
    "    Compute statistics for BLEU.\n",
    "    Args:\n",
    "    hypothesis: T*B, T is the longest length, B is batch size\n",
    "    \"\"\"\n",
    "    stats = stats.view(n,2)\n",
    "    precision = stats[:,0]/stats[:, 1]\n",
    "    print(precision)\n",
    "    precision = precision.log().mean()\n",
    "    base = min(0, 1-r/c)\n",
    "    return 100*math.exp(base+precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_interact(source, encoder, decoder, sample, use_cuda):\n",
    "    with torch.no_grad():\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        stats = torch.zeros(8)\n",
    "        source = Variable(source.unsqueeze(1))\n",
    "        source_lens = [source.size(0)]\n",
    "        if use_cuda: source = source.cuda()\n",
    "        encoder_outputs, encoder_last_hidden = encoder(source, source_lens, None)\n",
    "        batch_size = 1\n",
    "        max_target_len = 20\n",
    "        decoder_hidden = encoder_last_hidden\n",
    "        target_slice = Variable(torch.zeros(batch_size).fill_(test_data.target_vocab.SOS).long())\n",
    "        pred_seq = torch.zeros(20, 1).long()\n",
    "        pred_lens = torch.ones(batch_size).byte()\n",
    "        end = torch.zeros(batch_size).byte()\n",
    "        attns = torch.zeros(batch_size, max_target_len, source_lens[0])\n",
    "        if use_cuda:\n",
    "            source = source.cuda()\n",
    "            target_slice = target_slice.cuda()\n",
    "            pred_seq = pred_seq.cuda()\n",
    "            end = end.cuda()\n",
    "            pred_lens = pred_lens.cuda()\n",
    "        for l in range(max_target_len):\n",
    "            predictions, decoder_hidden, atten_scores = decoder(target_slice,\n",
    "                                                                encoder_outputs,\n",
    "                                                                source_lens,\n",
    "                                                                decoder_hidden)\n",
    "            attns[:, l, :] = atten_scores.squeeze(1)\n",
    "            if not sample:\n",
    "                pred_words = predictions.data.topk(5, 1)[1]\n",
    "                first = pred_words[:, 0]\n",
    "                second = pred_words[:, 1]\n",
    "                pred_words = torch.where(first == test_data.target_vocab.UNK, second, first)\n",
    "                pred_seq[l] = pred_words\n",
    "            else:\n",
    "                predictions[:, 0] = 0 # do not sample UNK\n",
    "                prob = torch.cumsum(softmax(predictions, 1), 1)\n",
    "                dice = torch.rand(batch_size).unsqueeze(1)\n",
    "                if use_cuda: dice = dice.cuda()\n",
    "                prob -= dice\n",
    "                prob = prob > 0\n",
    "                pred_words = torch.zeros(batch_size).long()\n",
    "                if use_cuda: pred_words = pred_words.cuda()\n",
    "                for i in range(batch_size):\n",
    "                    first = prob[i, :].nonzero()[0]\n",
    "                    pred_words[i] = first\n",
    "                pred_seq[l] = pred_words\n",
    "            target_slice = Variable(pred_words)\n",
    "            eos = torch.eq(pred_seq[l], test_data.target_vocab.EOS)\n",
    "            end = (end + eos.cuda())>0\n",
    "            pred_lens += (end == 0)\n",
    "        for i in range(batch_size):\n",
    "            pred_sen = pred_seq[:pred_lens[i]-1, i]\n",
    "        for i in range(batch_size):\n",
    "#             print(\"Given source sequence:\\n {}\".format(vocab.to_text(source.data[:source_lens[i], i])))\n",
    "            print(\"{} sequence is:\\n {}\".format(\"Sample\" if sample else \"Greedy\", vocab.to_text(pred_seq[:pred_lens[i], i])))\n",
    "    return attns.squeeze(0)[:pred_lens[0]], vocab.to_text(pred_seq[:pred_lens[0], 0]).split()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy sequence is:\n",
      " he 's not a god <end>\n"
     ]
    }
   ],
   "source": [
    "context = \"is david bowie the real god .\"\n",
    "source = vocab.to_vec(context)\n",
    "attns, response = greedy_interact(source, encoder, decoder, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_interact(encoder, decoder, source, beam_size, vocab, alpha, n_best, \n",
    "                encoder_reverse=None, decoder_reverse=None, intra=0, repeat=3):\n",
    "    with torch.no_grad():\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        source = Variable(source.unsqueeze(1))\n",
    "        if args.cuda: source = source.cuda()\n",
    "        source_lens = [source.size(0)]\n",
    "        batch_size = 1\n",
    "        encoder_outputs, encoder_last_hidden = encoder(source, source_lens, None)\n",
    "        attns = torch.zeros(batch_size, args.global_max_target_len, source_lens[0])\n",
    "        decoder_hidden = encoder_last_hidden\n",
    "        make_beam = lambda : Beam(beam_size, vocab, alpha, n_best, args.cuda, intra, repeat)\n",
    "        beams = [make_beam() for _ in range(batch_size)]\n",
    "        decoder_hidden = (decoder_hidden[0].repeat(1,beam_size,1), decoder_hidden[1].repeat(1,beam_size,1))\n",
    "        encoder_outputs = encoder_outputs.repeat(1,beam_size,1)\n",
    "        source_lens = torch.LongTensor(source_lens).repeat(1,beam_size,1).view(-1).tolist()\n",
    "        pred_seq = torch.zeros(args.global_max_target_len, batch_size).long()\n",
    "        pred_lens = []\n",
    "        for l in range(args.global_max_target_len):\n",
    "            last_words = torch.stack([b.get_last_words() for b in beams])\n",
    "            last_words = Variable(last_words).t().contiguous().view(1, -1).squeeze(0).long()\n",
    "            if args.cuda: last_words = last_words.cuda()\n",
    "            logits, decoder_hidden, atten_scores = decoder(last_words,\n",
    "                                                                encoder_outputs,\n",
    "                                                                source_lens,\n",
    "                                                                decoder_hidden)\n",
    "            logits = log_softmax(logits, 1)\n",
    "            logits = logits.view(beam_size, batch_size, -1)\n",
    "            atten_scores = atten_scores.view(beam_size, batch_size, -1)\n",
    "\n",
    "            for j, b in enumerate(beams):\n",
    "                b.advance(logits[:, j], atten_scores.data[:, j])\n",
    "                last_roots = b.get_last_root()\n",
    "                for d in decoder_hidden:\n",
    "                    layer_size = d.size(0)\n",
    "                    beam_batch = d.size(1)\n",
    "                    hidden_size = d.size(2)\n",
    "                    sent_states = d.view(layer_size, beam_size, beam_batch // beam_size,\n",
    "                            hidden_size)[:, :, j]\n",
    "                    sent_states.data.copy_(sent_states.data.index_select(1, last_roots))\n",
    "        for i in range(batch_size):\n",
    "            pred, attn_beams = beams[i].topk(n_best)\n",
    "            if encoder_reverse is None or decoder_reverse is None:\n",
    "                pred_sen = torch.LongTensor(pred[0][1:])\n",
    "            else:\n",
    "                encoder_reverse.eval()\n",
    "                decoder_reverse.eval()\n",
    "                beam_first = pred[0][1:]\n",
    "                pred_lens = [len(i[1:]) for i in pred]\n",
    "                total = min(len(pred), n_best)\n",
    "                pred_source = torch.zeros(max(pred_lens), total).long()\n",
    "                for idx, p in enumerate(pred):\n",
    "                    pred_source[:pred_lens[idx], idx] = torch.LongTensor(p[1:])\n",
    "                pred_source = Variable(pred_source)\n",
    "                if args.cuda: pred_source = pred_source.cuda()\n",
    "                sorted_idx = np.argsort(pred_lens)\n",
    "                sorted_idx = sorted_idx[::-1].tolist()\n",
    "                pred_source = pred_source[:, sorted_idx]\n",
    "                pred_lens = [pred_lens[i] for i in sorted_idx]\n",
    "                encoder_reverse_outputs, encoder_reverse_last_hidden = encoder_reverse(pred_source, pred_lens, None)\n",
    "                decoder_reverse_hidden = encoder_reverse_last_hidden\n",
    "                loss = torch.zeros(total)\n",
    "                last_words = Variable(torch.zeros(total).fill_(vocab.SOS).long())\n",
    "                if args.cuda: \n",
    "                    last_words = last_words.cuda()\n",
    "                    loss = loss.cuda()\n",
    "                for l in range(source_lens[i]):\n",
    "                    logits, decoder_reverse_hidden, _ = decoder_reverse(last_words, \n",
    "                                                                       encoder_reverse_outputs,\n",
    "                                                                       pred_lens,\n",
    "                                                                       decoder_reverse_hidden)\n",
    "                    logits = log_softmax(logits, 1)\n",
    "                    loss += logits[:, source[:, i][l]]\n",
    "                    last_words = source[:, i][l].expand_as(last_words)\n",
    "                _, best_idx = torch.max(loss, 0)\n",
    "                pred_sen = pred_source[:, best_idx][0:pred_lens[best_idx]]\n",
    "                pred_sen = pred_sen.cpu()\n",
    "            pred_seq[0:len(pred_sen), i] = pred_sen\n",
    "            attns[i, 0:len(pred_sen)] = attn_beams[sorted_idx[best_idx]][1:]\n",
    "            pred_lens.append(len(pred_sen))\n",
    "#             print(\"Given source sequence:\\n {}\".format(vocab.to_text(source.data[:source_lens[i], i])))\n",
    "            print(\"MMI sequence was the {}th sentence in beam, and it is:\\n {}\".format(\n",
    "                sorted_idx[best_idx], vocab.to_text(pred_sen)))\n",
    "            print(\"First sentence in beam, and it is:\\n {} \\n\".format(vocab.to_text(beam_first)))\n",
    "            return attns.squeeze(0), vocab.to_text(pred_sen).split()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy sequence is:\n",
      " i 'm not going to let you down <end>\n",
      "Sample sequence is:\n",
      " the brotherhood at the pigheaded institute UNknown last night <end>\n",
      "Total number of candidates are 733\n",
      "MMI sequence was the 552th sentence in beam, and it is:\n",
      " this is the way it is <end>\n",
      "First sentence in beam, and it is:\n",
      " i don 't know what you 'r e talking about <end> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "context = \"nothing is impossible .\"\n",
    "source = vocab.to_vec(context)\n",
    "greedy_interact(source, encoder, decoder, False, True)\n",
    "greedy_interact(source, encoder, decoder, True, True)\n",
    "attns, response = beam_interact(encoder, decoder, source, 200, vocab, 0, 600, \n",
    "                                          encoder_reverse, decoder_reverse, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0,0.5,'i'),\n",
       " Text(0,1.5,'don'),\n",
       " Text(0,2.5,\"'t\"),\n",
       " Text(0,3.5,'know'),\n",
       " Text(0,4.5,'but'),\n",
       " Text(0,5.5,'i'),\n",
       " Text(0,6.5,'don'),\n",
       " Text(0,7.5,\"'t\"),\n",
       " Text(0,8.5,'want'),\n",
       " Text(0,9.5,'to'),\n",
       " Text(0,10.5,'be'),\n",
       " Text(0,11.5,'late'),\n",
       " Text(0,12.5,'<end>')]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe360c371d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(attns[:len(response)].tolist(), index=response, columns=context.split())\n",
    "f = sns.heatmap(df)\n",
    "f.set_xticklabels(f.get_xticklabels(), rotation=30, fontsize=8)\n",
    "f.set_yticklabels(f.get_yticklabels(), fontsize=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.get_figure().savefig(\"attention_figures/attention-3.png\", dpi=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy(test_loader, encoder, decoder, sample, use_cuda):\n",
    "    with torch.no_grad():\n",
    "        stats = torch.zeros(8)\n",
    "        c_acc, r_acc = 0, 0\n",
    "        count = Counter([])\n",
    "        count2 = Counter([])\n",
    "        with progressbar.ProgressBar(max_value=len(test_loader)) as bar:\n",
    "            for batch_id ,(source, source_lens, target, target_lens)in enumerate(test_loader):\n",
    "                bar.update(batch_id)\n",
    "                source, target = Variable(source), Variable(target)\n",
    "                if use_cuda: source, target = source.cuda(), target.cuda()\n",
    "                batch_size = source.size()[1]\n",
    "                encoder_outputs, encoder_last_hidden = encoder(source, source_lens, None)\n",
    "                max_target_len = max(target_lens)\n",
    "                decoder_hidden = encoder_last_hidden\n",
    "                target_slice = Variable(torch.zeros(batch_size).fill_(test_data.target_vocab.SOS).long())\n",
    "                pred_seq = torch.zeros_like(target.data)\n",
    "                pred_lens = torch.ones(batch_size).byte()\n",
    "                end = torch.zeros(batch_size).byte()\n",
    "                attns = torch.zeros(batch_size, max_target_len, source_lens[0])\n",
    "                if use_cuda:\n",
    "                    source, target = source.cuda(), target.cuda()\n",
    "                    target_slice = target_slice.cuda()\n",
    "                    pred_seq = pred_seq.cuda()\n",
    "                    end = end.cuda()\n",
    "                    pred_lens = pred_lens.cuda()\n",
    "                for l in range(max_target_len):\n",
    "                    predictions, decoder_hidden, atten_scores = decoder(target_slice,\n",
    "                                                                        encoder_outputs,\n",
    "                                                                        source_lens,\n",
    "                                                                        decoder_hidden)\n",
    "                    attns[:, l, :] = atten_scores.squeeze(1)\n",
    "                    if not sample:\n",
    "                        pred_words = predictions.data.topk(2, 1)[1]\n",
    "                        first = pred_words[:, 0]\n",
    "                        second = pred_words[:, 1]\n",
    "                        pred_words = torch.where(first == test_data.target_vocab.UNK, second, first)\n",
    "                        pred_seq[l] = pred_words\n",
    "                    else:\n",
    "                        predictions[:, 0] = 0 # do not sample UNK\n",
    "                        prob = torch.cumsum(softmax(predictions), 1)\n",
    "                        dice = torch.rand(batch_size).unsqueeze(1)\n",
    "                        if use_cuda: dice = dice.cuda()\n",
    "                        prob -= dice\n",
    "                        prob = prob > 0\n",
    "                        pred_words = torch.zeros(batch_size).long()\n",
    "                        if use_cuda: pred_words = pred_words.cuda()\n",
    "                        for i in range(batch_size):\n",
    "                            first = prob[i, :].nonzero()[0]\n",
    "                            pred_words[i] = first\n",
    "                        pred_seq[l] = pred_words\n",
    "                    target_slice = Variable(pred_words)\n",
    "                    eos = torch.eq(pred_seq[l], test_data.target_vocab.EOS)\n",
    "                    end = (end + eos.cuda())>0\n",
    "                    pred_lens += (end == 0)\n",
    "                for i in range(batch_size):\n",
    "                    pred_sen = pred_seq[:pred_lens[i]-1, i]\n",
    "                    count.update(pred_sen.tolist())\n",
    "                    count2.update([tuple(pred_sen.tolist()[i:i+2]) for i in range(len(pred_sen.tolist())-1)])\n",
    "                s, c, r = get_stats(pred_seq.cpu(), pred_lens.tolist(), target.data.cpu(), target_lens, 4)\n",
    "                stats += s\n",
    "                c_acc += c\n",
    "                r_acc += r\n",
    "                for i in range(batch_size):\n",
    "#                     if test_data.target_vocab.UNK in source.data[:source_lens[i], i]: continue\n",
    "                    print(\"Given source sequence:\\n {}\".format(vocab.to_text(source.data[:source_lens[i], i])))\n",
    "                    print(\"target sequence is:\\n {}\".format(vocab.to_text(target.data[:target_lens[i], i])))\n",
    "                    print(\"greedily decoded sequence is:\\n {}\".format(vocab.to_text(pred_seq[:pred_lens[i], i])))\n",
    "                    print(\"attention scores is: \\n {}\".format(attns[i, :, :]))\n",
    "                raise NotImplementedError\n",
    "            bleus = []\n",
    "            distinct1 = len(count.keys())/sum(count.values())\n",
    "            print(\"total words are {}, total 2-grams are {}\".format(sum(count.values()), sum(count2.values())))\n",
    "            distinct2 = len(count2.keys())/sum(count2.values())\n",
    "            for i in range(1,5):\n",
    "                bleus.append(compute_bleu(i, c_acc, r_acc, stats[0:2*i]))\n",
    "            return bleus, distinct1, distinct2\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleus, distinct1, distinct2 = greedy(test_loader, encoder, decoder, False, True)\n",
    "bleus, distinct1, distinct2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = Tensor([0.1, 0.2, 0.3, 0.4, 0.2])\n",
    "torch.cumsum(A, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Beam():\n",
    "    def __init__(self, beam_size, vocab, alpha, n_best, use_cuda, intra=0, repeat=0):\n",
    "        self.beam_size = beam_size\n",
    "        self.vocab = vocab\n",
    "        self.alpha = alpha\n",
    "        self.n_best = n_best\n",
    "\n",
    "        self.prevs = [] # pointer to sequence in beam\n",
    "        self.nexts = [torch.zeros(beam_size).fill_(vocab.SOS)]\n",
    "        if use_cuda: self.nexts = [t.cuda() for t in self.nexts]\n",
    "        self.attns = []\n",
    "        self.scores = torch.zeros(beam_size)\n",
    "        self.all_scores = []\n",
    "        if use_cuda: self.scores = self.scores.cuda()\n",
    "        self.finished = [] # list of tuples, (index within beam, output index, score)\n",
    "        self.stop = False\n",
    "        self.cuda = use_cuda\n",
    "        self.intra = intra\n",
    "        assert 0 <= repeat <= 5\n",
    "        self.repeat = repeat\n",
    "\n",
    "    def get_last_words(self):\n",
    "        return self.nexts[-1]\n",
    "\n",
    "    def get_last_root(self):\n",
    "        return self.prevs[-1]\n",
    "\n",
    "    \n",
    "    def get_pred(self, word_idx, beam_idx):\n",
    "        pred = []\n",
    "        attn = []\n",
    "        for i in range(len(self.prevs[:beam_idx]), -1, -1):\n",
    "            pred.append(self.nexts[i][word_idx].item())\n",
    "            attn.append(self.attns[i-1][word_idx])\n",
    "            word_idx = self.prevs[i-1][word_idx]\n",
    "        attn.reverse()\n",
    "        pred.reverse()\n",
    "        return pred, torch.stack(attn)\n",
    "    \n",
    "    def advance(self, logits, attn):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        `logits`: log probability of each candidate sequence for generating next word, beam_size x vocab_size\n",
    "        `attn`: attention vectors of decoder\n",
    "        \"\"\"\n",
    "        has_repeat = False\n",
    "        if len(self.prevs) == 0:\n",
    "            beam_scores = logits[0]\n",
    "        else:\n",
    "            c = Counter()\n",
    "            penal = []\n",
    "            # intra-sibling penalization\n",
    "            for i in range(len(self.prevs[-1])):\n",
    "                c[self.prevs[-1][i]]+=1\n",
    "                penal.append(c[self.prevs[-1][i]]-1)\n",
    "            penal = Tensor(penal).cuda() if self.cuda else Tensor(penal)\n",
    "            penal *= self.intra\n",
    "            self.scores -= penal\n",
    "            beam_scores = self.scores.unsqueeze(1).expand_as(logits) + logits\n",
    "            for i in range(self.nexts[-1].size(0)):\n",
    "                if self.nexts[-1][i] == self.vocab.EOS or self.nexts[-1][i] == self.vocab.UNK:\n",
    "                    beam_scores[i] = -1e20\n",
    "                if self.repeat > 0: \n",
    "                    hyp, _ = self.get_pred(len(self.nexts)-1, i)\n",
    "                    ngrams = [tuple(hyp[j:j+self.repeat]) for j in range(len(hyp)+1-self.repeat)]\n",
    "                    c = Counter(ngrams)\n",
    "                    has_repeat = np.any(np.array(list(c.values())) > 1)\n",
    "                    if has_repeat: \n",
    "                        vocab.to_text(hyp)\n",
    "                        beam_scores[i] = -1e20\n",
    "\n",
    "        flat_beam_scores = beam_scores.view(-1)\n",
    "        best_scores, best_word_id = flat_beam_scores.topk(self.beam_size, 0, True, True)\n",
    "        self.all_scores.append(self.scores)\n",
    "        self.scores = best_scores\n",
    "        prev = best_word_id / self.vocab.vocab_size\n",
    "        prev = prev.data.long()\n",
    "        self.prevs.append(prev)\n",
    "        next_idx = (best_word_id % self.vocab.vocab_size).data.long()\n",
    "        if next_idx.is_cuda: next_idx = next_idx.cpu()\n",
    "        self.nexts.append(next_idx)\n",
    "        self.attns.append(attn.index_select(0, prev))\n",
    "        for idx, word_idx in enumerate(self.nexts[-1]):\n",
    "            if word_idx == self.vocab.EOS:\n",
    "                self.finished.append((idx, len(self.nexts)-1, (self.scores.data[idx])/(len(self.nexts)-1)))\n",
    "        if self.nexts[-1][0] == self.vocab.EOS:\n",
    "            self.all_scores.append(self.scores)\n",
    "            self.stop = True\n",
    "\n",
    "    def topk(self, k):\n",
    "        \"\"\"\n",
    "        If this beam has finished searching, get the top k best sequence. If there are less than k completed sentences,\n",
    "        add partial sentences.\n",
    "        \"\"\"\n",
    "        self.finished.sort(key=lambda x : -x[2]) #TODO: Check why this is inverse\n",
    "        scores = [s for _, _, s in self.finished]\n",
    "        print(\"Total number of candidates are {}\".format(len(self.finished)))\n",
    "        idx = [(word_idx, beam_idx) for (word_idx, beam_idx, _) in self.finished[0:k]]\n",
    "        makeup = k-len(idx)\n",
    "        for i, (score, word_idx) in enumerate(zip(self.scores, self.nexts[-1])):\n",
    "            if i > makeup -1: continue\n",
    "            scores.append(score/(len(self.nexts)-1))\n",
    "            idx.append((i, len(self.nexts)-1))\n",
    "\n",
    "        preds = [self.get_pred(*x) for x in idx]\n",
    "        sentences, attns = zip(*preds)\n",
    "        return sentences, attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(encoder, decoder, test_loader, beam_size, vocab, alpha, n_best, \n",
    "                encoder_reverse=None, decoder_reverse=None, intra=0, repeat=3):\n",
    "    with torch.no_grad():\n",
    "        stats = torch.zeros(8)\n",
    "        c_acc, r_acc = 0, 0\n",
    "        count = Counter([])\n",
    "        count2 = Counter([])\n",
    "        with progressbar.ProgressBar(max_value=len(test_loader)) as bar:\n",
    "            for batch_id ,(source, source_lens, target, target_lens)in enumerate(test_loader):\n",
    "                bar.update(batch_id)\n",
    "                source, target = Variable(source), Variable(target)\n",
    "                if args.cuda: source, target = source.cuda(), target.cuda()\n",
    "                batch_size = source.size()[1]\n",
    "                encoder_outputs, encoder_last_hidden = encoder(source, source_lens, None)\n",
    "                decoder_hidden = encoder_last_hidden\n",
    "                make_beam = lambda : Beam(beam_size, vocab, alpha, n_best, args.cuda, intra, repeat)\n",
    "                beams = [make_beam() for _ in range(batch_size)]\n",
    "                decoder_hidden = (decoder_hidden[0].repeat(1,beam_size,1), decoder_hidden[1].repeat(1,beam_size,1))\n",
    "                encoder_outputs = encoder_outputs.repeat(1,beam_size,1)\n",
    "                source_lens = torch.LongTensor(source_lens).repeat(1,beam_size,1).view(-1).tolist()\n",
    "                pred_seq = torch.zeros(30, target.size()[1]).long()\n",
    "                pred_lens = []\n",
    "                for l in range(args.global_max_target_len):\n",
    "                    last_words = torch.stack([b.get_last_words() for b in beams])\n",
    "                    last_words = Variable(last_words).t().contiguous().view(1, -1).squeeze(0).long()\n",
    "                    if args.cuda: last_words = last_words.cuda()\n",
    "                    logits, decoder_hidden, atten_scores = decoder(last_words,\n",
    "                                                                        encoder_outputs,\n",
    "                                                                        source_lens,\n",
    "                                                                        decoder_hidden)\n",
    "                    logits = log_softmax(logits, 1)\n",
    "                    logits = logits.view(beam_size, batch_size, -1)\n",
    "                    atten_scores = atten_scores.view(beam_size, batch_size, -1)\n",
    "\n",
    "                    for j, b in enumerate(beams):\n",
    "                        b.advance(logits[:, j], atten_scores.data[:, j])\n",
    "                        last_roots = b.get_last_root()\n",
    "                        for d in decoder_hidden:\n",
    "                            layer_size = d.size(0)\n",
    "                            beam_batch = d.size(1)\n",
    "                            hidden_size = d.size(2)\n",
    "                            sent_states = d.view(layer_size, beam_size, beam_batch // beam_size,\n",
    "                                    hidden_size)[:, :, j]\n",
    "                            sent_states.data.copy_(sent_states.data.index_select(1, last_roots))\n",
    "                for i in range(batch_size):\n",
    "                    pred, _ = beams[i].topk(n_best)\n",
    "                    if encoder_reverse is None or decoder_reverse is None:\n",
    "                        pred_sen = torch.LongTensor(pred[0][1:])\n",
    "                    else:\n",
    "                        beam_first = pred[0][1:]\n",
    "                        pred_lens = [len(i[1:]) for i in pred]\n",
    "                        total = min(len(pred), n_best)\n",
    "                        pred_source = torch.zeros(max(pred_lens), total).long()\n",
    "                        for idx, p in enumerate(pred):\n",
    "                            pred_source[:pred_lens[idx], idx] = torch.LongTensor(p[1:])\n",
    "                        pred_source = Variable(pred_source)\n",
    "                        if args.cuda: pred_source = pred_source.cuda()\n",
    "                        sorted_idx = np.argsort(pred_lens)\n",
    "                        sorted_idx = sorted_idx[::-1].tolist()\n",
    "                        pred_source = pred_source[:, sorted_idx]\n",
    "                        pred_lens = [pred_lens[i] for i in sorted_idx]\n",
    "                        encoder_reverse_outputs, encoder_reverse_last_hidden = encoder_reverse(pred_source, pred_lens, None)\n",
    "                        decoder_reverse_hidden = encoder_reverse_last_hidden\n",
    "                        loss = torch.zeros(total)\n",
    "                        last_words = Variable(torch.zeros(total).fill_(vocab.SOS).long())\n",
    "                        if args.cuda: \n",
    "                            last_words = last_words.cuda()\n",
    "                            loss = loss.cuda()\n",
    "                        for l in range(source_lens[i]):\n",
    "                            logits, decoder_reverse_hidden, _ = decoder_reverse(last_words, \n",
    "                                                                               encoder_reverse_outputs,\n",
    "                                                                               pred_lens,\n",
    "                                                                               decoder_reverse_hidden)\n",
    "                            logits = log_softmax(logits, 1)\n",
    "                            loss += logits[:, source[:, i][l]]\n",
    "                            last_words = source[:, i][l].expand_as(last_words)\n",
    "                        _, best_idx = torch.max(loss, 0)\n",
    "                        pred_sen = pred_source[:, best_idx][0:pred_lens[best_idx]]\n",
    "                        pred_sen = pred_sen.cpu()\n",
    "                    count.update(pred_sen.tolist()[:-1])\n",
    "                    count2.update([tuple(pred_sen.tolist()[i:i+2]) for i in range(len(pred_sen)-2)])\n",
    "                    pred_seq[0:len(pred_sen), i] = pred_sen\n",
    "                    pred_lens.append(len(pred_sen))\n",
    "                    print(\"Given source sequence:\\n {}\".format(vocab.to_text(source.data[:source_lens[i], i])))\n",
    "                    print(\"target sequence is:\\n {}\".format(vocab.to_text(target.data[:target_lens[i], i])))\n",
    "                    print(\"MMI sequence was the {}th sentence in beam, and it is:\\n {}\".format(\n",
    "                        sorted_idx[best_idx], vocab.to_text(pred_sen)))\n",
    "                    print(\"First sentence in beam, and it is:\\n {}\".format(vocab.to_text(beam_first)))\n",
    "#                     print(\"beam search sequence is:\")\n",
    "#                     for i in range(n_best):\n",
    "#                         print(\"{}. {}\".format(i, vocab.to_text(pred[i])))\n",
    "                    raise NotImplementedError\n",
    "                s, c, r = get_stats(pred_seq.cpu(), pred_lens, target.data.cpu(), target_lens, 4)\n",
    "                stats += s\n",
    "                c_acc += c\n",
    "                r_acc += r\n",
    "\n",
    "        bleus = []\n",
    "        distinct1 = len(count.keys())/sum(count.values())\n",
    "        distinct2 = len(count2.keys())/sum(count2.values())\n",
    "        print(\"total words are {}, total 2-grams are {}\".format(sum(count.values()), sum(count2.values())))\n",
    "        for i in range(1,5):\n",
    "            bleus.append(compute_bleu(i, c_acc, r_acc, stats[0:2*i]))\n",
    "        return bleus, distinct1, distinct2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bleus, distinct1, distinct2 = greedy(test_loader, encoder, decoder, args.cuda)\n",
    "bleus, distinct1, distinct2 = beam_search(encoder, decoder, test_loader, 200, vocab, 0, 1000, \n",
    "                                          encoder_reverse, decoder_reverse, 0, 0)\n",
    "print(\"MMI\")\n",
    "print(bleus, distinct1, distinct2)\n",
    "# bleus, distinct1, distinct2 = beam_search(encoder, decoder, test_loader, 200, vocab, 0, 200, \n",
    "#                                           None, None, 0, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Beam200\")\n",
    "print(bleus, distinct1, distinct2)\n",
    "bleus, distinct1, distinct2 = beam_search(encoder, decoder, test_loader, 200, vocab, 0, 200, \n",
    "                                          None, None, 1, 0)\n",
    "print(\"Intra\")\n",
    "print(bleus, distinct1, distinct2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.to_text([5, 32, 13, 46])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleus, distinct1, distinct2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bleu2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [1, 1, 2, 3, 2]\n",
    "c = Counter()\n",
    "p = []\n",
    "for i in range(len(l)):\n",
    "    c[l[i]]+=1\n",
    "    p.append(c[l[i]]-1)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
