{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor, LongTensor\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from data import  CornellVocab, CornellMovie, OpenSubVocab, OpenSub, sort_batch\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from utils import parse, length_to_mask, masked_cross_entropy_loss, save_checkpoint\n",
    "from tensorboardX import SummaryWriter\n",
    "from seq import Encoder, Decoder, run\n",
    "# from inference import Beam\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "from time import strftime, localtime, time\n",
    "from torch.nn.functional import softmax, log_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Beam():\n",
    "    def __init__(self, beam_size, vocab, alpha, n_best, use_cuda):\n",
    "        self.beam_size = beam_size\n",
    "        self.vocab = vocab\n",
    "        self.alpha = alpha\n",
    "        self.n_best = n_best\n",
    "\n",
    "        self.prevs = [] # pointer to sequence in beam\n",
    "        self.nexts = [torch.zeros(beam_size).fill_(vocab.SOS)]\n",
    "        if use_cuda: self.nexts = [t.cuda() for t in self.nexts]\n",
    "        self.attns = []\n",
    "        self.scores = torch.zeros(beam_size)\n",
    "        self.all_scores = []\n",
    "        if use_cuda: self.scores = self.scores.cuda()\n",
    "        self.finished = [] # list of tuples, (index within beam, output index, score)\n",
    "        self.stop = False\n",
    "\n",
    "    def get_last_words(self):\n",
    "        return self.nexts[-1]\n",
    "    \n",
    "    def get_last_root(self):\n",
    "        return self.prevs[-1]\n",
    "\n",
    "    def advance(self, logits, attn):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        `logits`: log probability of each candidate sequence for generating next word, beam_size x vocab_size\n",
    "        `attn`: attention vectors of decoder\n",
    "        \"\"\"\n",
    "        if len(self.prevs) == 0:\n",
    "            beam_scores = logits[0]\n",
    "        else:\n",
    "            beam_scores = self.scores.unsqueeze(1).expand_as(logits) + logits\n",
    "            for i in range(self.nexts[-1].size(0)):\n",
    "                if self.nexts[-1][i] == self.vocab.EOS:\n",
    "                    beam_scores[i] = -1e20 \n",
    "        print(beam_scores)\n",
    "                \n",
    "        \n",
    "        \n",
    "            #TODO: Block Children of finish sentence\n",
    "            #TODO: Normalization over length\n",
    "\n",
    "        flat_beam_scores = beam_scores.view(-1)\n",
    "        best_scores, best_word_id = flat_beam_scores.topk(self.beam_size, 0, True, True)\n",
    "        print(best_scores)\n",
    "        print(best_word_id)\n",
    "        self.all_scores.append(self.scores)\n",
    "        self.scores = best_scores\n",
    "        prev = best_word_id / self.vocab.vocab_size\n",
    "        prev = prev.data.long()\n",
    "        self.prevs.append(prev)\n",
    "        next_idx = (best_word_id % self.vocab.vocab_size).data.long()\n",
    "        self.nexts.append(next_idx)\n",
    "        self.attns.append(attn.index_select(0, prev))\n",
    "        for idx, word_idx in enumerate(self.nexts[-1]):\n",
    "            if word_idx == self.vocab.EOS:\n",
    "                self.finished.append((idx, len(self.nexts)-1, self.scores.data[idx]))\n",
    "        if self.nexts[-1][0] == self.vocab.EOS:\n",
    "            self.all_scores.append(self.scores)\n",
    "            self.stop = True\n",
    "\n",
    "    def topk(self, k):\n",
    "        \"\"\"\n",
    "        If this beam has finished searching, get the top k best sequence. If there are less than k completed sentences,\n",
    "        add partial sentences.\n",
    "        \"\"\"\n",
    "        self.finished.sort(key=lambda x : x[2]) #TODO: Check why this is inverse\n",
    "        scores = [s for _, _, s in self.finished]\n",
    "        idx = [(word_idx, beam_idx) for (word_idx, beam_idx, _) in self.finished]\n",
    "        makeup = k-len(idx)\n",
    "        for i, (score, word_idx) in enumerate(zip(self.scores, self.nexts[-1])):\n",
    "            if i > makeup -1: continue\n",
    "            scores.append(score)\n",
    "            idx.append((i, len(self.nexts)-1))\n",
    "                       \n",
    "        def get_pred(word_idx, beam_idx):\n",
    "            pred = []\n",
    "            attn = []\n",
    "            for i in range(len(self.prevs[:beam_idx]), -1, -1):\n",
    "                pred.append(self.nexts[i][word_idx])\n",
    "                attn.append(self.attns[i-1][word_idx])\n",
    "                word_idx = self.prevs[i-1][word_idx]\n",
    "            attn.reverse()\n",
    "            pred.reverse()\n",
    "            return pred, torch.stack(attn)\n",
    "        preds = [get_pred(*x) for x in idx]\n",
    "        sentences, attns = zip(*preds)\n",
    "        return sentences, attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"epoch\":20,\n",
    "    \"batch_size\":1,\n",
    "    \"num_workers\":4,\n",
    "    \"train_path\":\"../data/dev/2020_dev\",\n",
    "    \"test_path\":\"../data/dev/2020_dev\",\n",
    "    \"vocab_size\":25000,\n",
    "    \"embed_size\":1000,\n",
    "    \"hidden_size\":1000,\n",
    "    \"num_layers\":4,\n",
    "    \"clip_thresh\":1,\n",
    "    \"seed\":1,\n",
    "    \"lr\":0.1,\n",
    "    \"global_max_target_len\":20,\n",
    "    \"cuda\":torch.cuda.is_available,\n",
    "    \"resume\":\"checkpoint-29999\",\n",
    "    \"dir\":\"final\",\n",
    "    \"reverse\": False,\n",
    "    \"vocab_path\":\"../data/movie_25000\",\n",
    "    \"dropout\":0.2\n",
    "}\n",
    "class AttributeDict(dict):\n",
    "    def __getattr__(self, attr):\n",
    "        return self[attr]\n",
    "    def __setattr__(self, attr, value):\n",
    "        self[attr] = value\n",
    "args = AttributeDict(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = OpenSubVocab(args.vocab_path)\n",
    "train_data = OpenSub(args, vocab, args.train_path)\n",
    "test_data = OpenSub(args, vocab, args.test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                           batch_size=args.batch_size,\n",
    "                                           shuffle=True, collate_fn=sort_batch,\n",
    "                                           num_workers=args.num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_data,\n",
    "                                          batch_size=args.batch_size,\n",
    "                                          shuffle=True, collate_fn=sort_batch,\n",
    "                                          num_workers=args.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(args, train_data.source_vocab.vocab_size).cuda() if args.cuda else Encoder(args, train_data.source_vocab.vocab_size)\n",
    "decoder = Decoder(args, train_data.target_vocab.vocab_size).cuda() if args.cuda else Decoder(args, train_data.target_vocab.vocab_size)\n",
    "\n",
    "\n",
    "encoder_optim = optim.SGD(encoder.parameters(), lr=args.lr)\n",
    "decoder_optim = optim.SGD(decoder.parameters(), lr=args.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(os.path.join(args.dir, args.resume))\n",
    "encoder.load_state_dict(checkpoint['encoder_state'])\n",
    "decoder.load_state_dict(checkpoint['decoder_state'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "source, source_lens, target, target_lens = next(iter(test_loader))\n",
    "source, target = Variable(source, volatile=True), Variable(target, volatile=True)\n",
    "if use_cuda: source, target = source.cuda(), target.cuda()\n",
    "batch_size = source.size()[1]\n",
    "encoder_outputs, encoder_last_hidden = encoder(source, source_lens, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_hidden = encoder_last_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tgt_len = 1\n",
    "beam_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_hidden = (decoder_hidden[0].repeat(1,beam_size,1), decoder_hidden[1].repeat(1,beam_size,1))\n",
    "encoder_outputs = encoder_outputs.repeat(1,beam_size,1)\n",
    "source_lens = torch.LongTensor(source_lens).repeat(1,beam_size,1).view(-1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_beam = lambda : Beam(beam_size, vocab, 0, 2, use_cuda)\n",
    "beams = [make_beam() for _ in range(batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0\n",
      "[torch.cuda.LongTensor of size 1 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      "-1.0000e+20 -1.0000e+20 -1.0000e+20  ...  -1.0000e+20 -1.0000e+20 -1.0000e+20\n",
      "[torch.cuda.FloatTensor of size 1x25004 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      "-1.0000e+20\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      " 0\n",
      "[torch.cuda.LongTensor of size 1 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for l in range(max_tgt_len):\n",
    "    last_words = torch.stack([b.get_last_words() for b in beams])\n",
    "    last_words = Variable(last_words).t().contiguous().view(1, -1).squeeze(0).long()\n",
    "    print(last_words)\n",
    "    if use_cuda: last_words = last_words.cuda()\n",
    "    logits, decoder_hidden, atten_scores = decoder(last_words,encoder_outputs,source_lens,decoder_hidden)\n",
    "    logits = log_softmax(logits, 1)\n",
    "    logits = logits.view(beam_size, batch_size, -1)\n",
    "    atten_scores = atten_scores.view(beam_size, batch_size, -1)\n",
    "\n",
    "    for j, b in enumerate(beams):\n",
    "        b.advance(logits[:, j], atten_scores.data[:, j])\n",
    "        last_roots = b.get_last_root()\n",
    "        for d in decoder_hidden:\n",
    "            layer_size = d.size(0)\n",
    "            beam_batch = d.size(1)\n",
    "            hidden_size = d.size(2)\n",
    "            sent_states = d.view(layer_size, beam_size, beam_batch // beam_size,\n",
    "                    hidden_size)[:, :, j]\n",
    "            sent_states.data.copy_(sent_states.data.index_select(1, last_roots))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      " 25002\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      ", \n",
      " 5\n",
      "[torch.cuda.LongTensor of size 1 (GPU 0)]\n",
      ", \n",
      " 39\n",
      "[torch.cuda.LongTensor of size 1 (GPU 0)]\n",
      ", \n",
      " 37\n",
      "[torch.cuda.LongTensor of size 1 (GPU 0)]\n",
      ", \n",
      " 136\n",
      "[torch.cuda.LongTensor of size 1 (GPU 0)]\n",
      ", \n",
      " 5\n",
      "[torch.cuda.LongTensor of size 1 (GPU 0)]\n",
      ", \n",
      " 46\n",
      "[torch.cuda.LongTensor of size 1 (GPU 0)]\n",
      ", \n",
      " 25001\n",
      "[torch.cuda.LongTensor of size 1 (GPU 0)]\n",
      ", \n",
      " 0\n",
      "[torch.cuda.LongTensor of size 1 (GPU 0)]\n",
      ", \n",
      " 0\n",
      "[torch.cuda.LongTensor of size 1 (GPU 0)]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(b.nexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       "  0\n",
       " [torch.cuda.LongTensor of size 1 (GPU 0)], \n",
       "  0\n",
       " [torch.cuda.LongTensor of size 1 (GPU 0)], \n",
       "  0\n",
       " [torch.cuda.LongTensor of size 1 (GPU 0)], \n",
       "  0\n",
       " [torch.cuda.LongTensor of size 1 (GPU 0)], \n",
       "  0\n",
       " [torch.cuda.LongTensor of size 1 (GPU 0)], \n",
       "  0\n",
       " [torch.cuda.LongTensor of size 1 (GPU 0)], \n",
       "  0\n",
       " [torch.cuda.LongTensor of size 1 (GPU 0)], \n",
       "  0\n",
       " [torch.cuda.LongTensor of size 1 (GPU 0)], \n",
       "  0\n",
       " [torch.cuda.LongTensor of size 1 (GPU 0)]]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.prevs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, _ = b.topk(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i 'm so glad we met her <end>\n",
      "how about taking a look at the lion she works with ? <end>\n",
      "<start>i 'm not sure i know <end>\n"
     ]
    }
   ],
   "source": [
    "print(test_data.target_vocab.to_text(source.data[:, 0]))\n",
    "print(test_data.target_vocab.to_text(target.data[:, 0]))\n",
    "print(test_data.target_vocab.to_text(preds[0]))\n",
    "# print(test_data.target_vocab.to_text(preds[1]))\n",
    "# print(test_data.target_vocab.to_text(preds[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
